# Se mettre dans le dossier "data_integration_kafka1" afin de executer les commandes

# lancer le serveur ssh
ssh localhost

# Démarrez NameNode et DataNode
start-dfs.sh

# lancer le zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# lancer le serveur kafka
~/Téléchargements/kafka37$ bin/kafka-server-start.sh config/server.properties 
                    ou
bin/kafka-server-start.sh config/server.properties


# lancer le consumer
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2 kafka/consumer.py

# lancer le producer
python kafka/producer.py

# lancer le spark Shell 
spark-shell

# Uploader les deux fichiers ui ne snt pas en streaming dans le HDFS pour effectuer les traitements
hdfs dfs -put /data/in-hospital-mortality-trends-by-diagnosis-type.csv /hospital_data/in-hospital-mortality-trends-by-diagnosis-type.csv
hdfs dfs -put /data/in-hospital-mortality-trends-by-health-category.csv /hospital_data/in-hospital-mortality-trends-by-health-category.csv